from transformers import pipeline
import torch

MODEL_PATH = "E:/Pycharm/LangChain/embedding"

try:
    # åˆ›å»ºåµŒå…¥ pipelinefrom langchain.agents import AgentExecutor, Tool, create_openai_functions_agent
    # from langchain_openai import ChatOpenAI
    # from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    # from langchain_community.chat_models import ChatOpenAI
    # import matplotlib.pyplot as plt
    # import pandas as pd
    # import numpy as np
    # from datetime import datetime, timedelta
    # import textwrap
    # import re
    # import os
    # from longbridge.openapi import QuoteContext, Config, SubType, Period, AdjustType, CalcIndex
    # from langchain.tools import Tool
    # from langchain_community.vectorstores import FAISS
    # import numpy as np
    # from datetime import datetime
    # from Embedding_Vector import Creat_Embeddings
    # from dotenv import load_dotenv
    #
    # load_dotenv()
    #
    # # åˆå§‹åŒ–Longbridgeé…ç½®
    # config = Config(
    #     app_key=os.getenv("LONGPORT_APP_KEY"),
    #     app_secret=os.getenv("LONGPORT_APP_SECRET"),
    #     access_token=os.getenv("LONGPORT_ACCESS_TOKEN")
    # )
    #
    # # é…ç½®è·¯å¾„
    # MODEL_PATH = "E:/Pycharm/LangChain/embedding"
    # FAISS_INDEX_PATH = "E:/Pycharm/LangChain/FAISS_Stock_Data"
    # INDEX_NAME = "hk_stock_index"
    #
    #
    # # åˆ›å»ºè‡ªå®šä¹‰åµŒå…¥å¯¹è±¡
    # embedding = Creat_Embeddings()
    #
    # # è‚¡ç¥¨åç§°åˆ°ä»£ç æ˜ å°„
    # name_to_code = {
    #     "è…¾è®¯": "00700.HK",
    #     "é˜¿é‡Œå·´å·´": "09988.HK",
    #     "ç¾å›¢": "03690.HK",
    #     "å°ç±³": "01810.HK",
    #     "ç²¤æ¸¯æ¹¾": "01396.HK",
    #     "äº¬ä¸œ": "09618.HK",
    #     "ç™¾åº¦": "09888.HK",
    #     "ç½‘æ˜“": "09999.HK",
    #     "æ¯”äºšè¿ª": "01211.HK",
    #     "ä¸­å›½å¹³å®‰": "02318.HK",
    # }
    #
    #
    # # åŠ è½½ FAISS å‘é‡åº“
    # def load_faiss_index():
    #     """åŠ è½½ FAISS å‘é‡åº“"""
    #     if not os.path.exists(os.path.join(FAISS_INDEX_PATH, f"{INDEX_NAME}.faiss")):
    #         print(f"æœªå­˜å‚¨{INDEX_NAME}ç›¸å…³æ•°æ®")
    #         return None
    #
    #     try:
    #         db = FAISS.load_local(FAISS_INDEX_PATH, embedding, index_name=INDEX_NAME, allow_dangerous_deserialization=True)
    #         print(f"å·²åŠ è½½è‚¡ç¥¨æ•°æ®ç´¢å¼•ï¼ŒåŒ…å« {db.index.ntotal} ä¸ªæ–‡æ¡£")
    #         return db
    #     except Exception as e:
    #         print(f"åŠ è½½ç´¢å¼•å¤±è´¥: {str(e)}")
    #         return None
    #
    # # åˆå§‹åŒ–å‘é‡åº“
    # vector_db = load_faiss_index()
    #
    #
    #
    # # 1. åŸºç¡€æœç´¢å·¥å…·
    # def stock_search_tool(query: str, k: int = 5) -> str:
    #     """åœ¨å‘é‡åº“ä¸­æœç´¢ç›¸å…³è‚¡ç¥¨æ•°æ®"""
    #     if not vector_db:
    #         return "è‚¡ç¥¨æ•°æ®ç´¢å¼•æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ•°æ®"
    #
    #     results = vector_db.similarity_search(query, k=k)
    #
    #     output = []
    #     for i, doc in enumerate(results):
    #         stock_name = doc.metadata.get("stock_name", "æœªçŸ¥è‚¡ç¥¨")
    #         stock_code = doc.metadata.get("stock_code", "æœªçŸ¥ä»£ç ")
    #         update_time = doc.metadata.get("update_time", "æœªçŸ¥æ—¶é—´")
    #
    #         # æ ¼å¼åŒ–æ–‡æœ¬æ‘˜è¦
    #         content = textwrap.fill(doc.page_content, width=80)
    #
    #         output.append(
    #             f"ğŸ” {i + 1}. {stock_name} ({stock_code})\n"
    #             f"ğŸ•’ æ›´æ–°æ—¶é—´: {update_time}\n"
    #             f"ğŸ“ æ‘˜è¦: {content}\n"
    #             f"{'-' * 40}"
    #         )
    #
    #     return "\n".join(output)
    #
    # # 2. è¶‹åŠ¿åˆ†æå·¥å…·
    # def trend_analysis_tool(stock_names: str, days: int = 5) -> str:
    #     """
    #     åˆ†æè‚¡ç¥¨è¶‹åŠ¿ï¼Œè¿”å›æ–‡æœ¬æŠ¥å‘Šå’Œå›¾è¡¨è·¯å¾„
    #     è¾“å…¥: è‚¡ç¥¨åç§°ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰ï¼Œåˆ†æå¤©æ•°
    #     è¾“å‡º: è¶‹åŠ¿åˆ†ææŠ¥å‘Šå’Œå›¾è¡¨æ–‡ä»¶è·¯å¾„
    #     """
    #     # 1. è®¾ç½®ä¸­æ–‡å­—ä½“
    #     plt.rcParams['font.sans-serif'] = ['SimHei']  # Windows
    #     plt.rcParams['axes.unicode_minus'] = False
    #
    #     stock_list = [name.strip() for name in stock_names.split(",")]
    #
    #     if not vector_db:
    #         return "è‚¡ç¥¨æ•°æ®ç´¢å¼•æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ•°æ®"
    #
    #     # æ”¶é›†æ‰€æœ‰ç›¸å…³æ–‡æ¡£
    #     all_docs = []
    #     for stock in stock_list:
    #         if stock not in name_to_code:
    #             continue
    #
    #         # æœç´¢è¯¥è‚¡ç¥¨çš„æ‰€æœ‰æ–‡æ¡£
    #         query = f"{stock} {name_to_code[stock]}"
    #         docs = vector_db.similarity_search(query, k=10)
    #         all_docs.extend(docs)
    #
    #     if not all_docs:
    #         return "æœªæ‰¾åˆ°ç›¸å…³è‚¡ç¥¨æ•°æ®"
    #
    #     # è§£æKçº¿æ•°æ®
    #     kline_data = {}
    #     for doc in all_docs:
    #         stock_code = doc.metadata.get("stock_code")
    #         if not stock_code:
    #             continue
    #
    #         # ä»æ–‡æ¡£å†…å®¹ä¸­æå–Kçº¿æ•°æ®
    #         content = doc.page_content
    #         kline_matches = re.findall(r"æœ€è¿‘Kçº¿:([^:]+):å¼€(\d+\.?\d*)/æ”¶(\d+\.?\d*)ï¼Œ", content)
    #
    #         if not kline_matches:
    #             continue
    #
    #         # ç»„ç»‡Kçº¿æ•°æ®
    #         if stock_code not in kline_data:
    #             kline_data[stock_code] = {
    #                 "dates": [],
    #                 "opens": [],
    #                 "closes": [],
    #                 "stock_name": doc.metadata.get("stock_name", stock_code)
    #             }
    #
    #         for match in kline_matches:
    #             date_str, open_price, close_price = match
    #             kline_data[stock_code]["dates"].append(date_str)
    #             kline_data[stock_code]["opens"].append(float(open_price))
    #             kline_data[stock_code]["closes"].append(float(close_price))
    #
    #     if not kline_data:
    #         return "æœªæ‰¾åˆ°Kçº¿æ•°æ®"
    #
    #     # åˆ›å»ºè¶‹åŠ¿å›¾è¡¨
    #     plt.figure(figsize=(12, 8))
    #
    #     for stock_code, data in kline_data.items():
    #         # åªå–æœ€è¿‘dayså¤©çš„æ•°æ®
    #         dates = data["dates"][-days:]
    #         closes = data["closes"][-days:]
    #
    #         if len(dates) < 2:
    #             continue
    #
    #         # è®¡ç®—ç§»åŠ¨å¹³å‡
    #         moving_avg = np.convolve(closes, np.ones(3) / 3, mode='valid')
    #
    #         plt.plot(dates, closes, 'o-', label=f"{data['stock_name']} ({stock_code})")
    #         plt.plot(dates[1:-1], moving_avg, '--', alpha=0.5)
    #
    #     plt.title(f"è‚¡ç¥¨ä»·æ ¼è¶‹åŠ¿ ({days}å¤©)")
    #     plt.xlabel("æ—¥æœŸ")
    #     plt.ylabel("æ”¶ç›˜ä»·")
    #     plt.legend()
    #     plt.grid(True)
    #     plt.xticks(rotation=45)
    #     plt.tight_layout()
    #
    #     # ä¿å­˜å›¾è¡¨
    #     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    #     chart_path = f"stock_trend_{timestamp}.png"
    #     plt.savefig(chart_path)
    #     plt.close()
    #
    #     # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    #     report = f"ğŸ“ˆ è‚¡ç¥¨è¶‹åŠ¿åˆ†ææŠ¥å‘Š ({days}å¤©)\n\n"
    #     report += f"åˆ†æè‚¡ç¥¨: {', '.join(stock_list)}\n"
    #     report += f"è¶‹åŠ¿å›¾è¡¨å·²ä¿å­˜è‡³: {chart_path}\n\n"
    #
    #     # æ·»åŠ ç®€è¦åˆ†æ
    #     report += "åˆ†ææ‘˜è¦:\n"
    #     for stock_code, data in kline_data.items():
    #         if len(data["closes"]) < 2:
    #             continue
    #
    #         latest_close = data["closes"][-1]
    #         prev_close = data["closes"][-2] if len(data["closes"]) > 1 else latest_close
    #         change = latest_close - prev_close
    #         change_percent = (change / prev_close) * 100 if prev_close != 0 else 0
    #
    #         trend = "ä¸Šæ¶¨ğŸ“ˆ" if change > 0 else "ä¸‹è·ŒğŸ“‰" if change < 0 else "æŒå¹³â–"
    #
    #         report += (
    #             f"- {data['stock_name']} ({stock_code}): "
    #             f"æœ€æ–°ä»· {latest_close:.2f} ({trend} {change_percent:.2f}%)\n"
    #         )
    #
    #     return report + f"\nå›¾è¡¨è·¯å¾„: {chart_path}"
    #
    # # print("===", trend_analysis_tool("è…¾è®¯"))
    #
    #
    # # 3. ç›¸å…³æ€§åˆ†æå·¥å…·
    # # def correlation_analysis_tool(stock_names: str) -> str:
    # #     """
    # #     åˆ†æå¤šä¸ªè‚¡ç¥¨ä¹‹é—´çš„ä»·æ ¼ç›¸å…³æ€§
    # #     è¾“å…¥: è‚¡ç¥¨åç§°ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰
    # #     è¾“å‡º: ç›¸å…³æ€§åˆ†ææŠ¥å‘Šå’Œå›¾è¡¨æ–‡ä»¶è·¯å¾„
    # #     """
    # #     stock_list = [name.strip() for name in stock_names.split(",")]
    # #
    # #     if not vector_db:
    # #         return "è‚¡ç¥¨æ•°æ®ç´¢å¼•æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ•°æ®"
    # #
    # #     # æ”¶é›†æ‰€æœ‰ç›¸å…³æ–‡æ¡£
    # #     stock_data = {}
    # #     for stock in stock_list:
    # #         if stock not in name_to_code:
    # #             continue
    # #
    # #         # æœç´¢è¯¥è‚¡ç¥¨çš„æ‰€æœ‰æ–‡æ¡£
    # #         query = f"{stock} {name_to_code[stock]}"
    # #         docs = vector_db.similarity_search(query, k=10)
    # #
    # #         # è§£ææ”¶ç›˜ä»·
    # #         closes = []
    # #         for doc in docs:
    # #             content = doc.page_content
    # #             kline_matches = re.findall(r":å¼€\d+\.?\d*/æ”¶(\d+\.?\d*)ï¼Œ", content)
    # #             if kline_matches:
    # #                 closes.extend([float(price) for price in kline_matches])
    # #
    # #         if closes:
    # #             stock_data[stock] = closes
    # #
    # #     if len(stock_data) < 2:
    # #         return "éœ€è¦è‡³å°‘ä¸¤æ”¯è‚¡ç¥¨è¿›è¡Œåˆ†æ"
    # #
    # #     # åˆ›å»ºç›¸å…³æ€§çŸ©é˜µ
    # #     min_length = min(len(prices) for prices in stock_data.values())
    # #     aligned_data = {stock: prices[-min_length:] for stock, prices in stock_data.items()}
    # #
    # #     df = pd.DataFrame(aligned_data)
    # #     corr_matrix = df.corr()
    # #
    # #     # åˆ›å»ºçƒ­åŠ›å›¾
    # #     plt.figure(figsize=(10, 8))
    # #     plt.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)
    # #     plt.colorbar()
    # #     plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=45)
    # #     plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)
    # #     plt.title('è‚¡ç¥¨ä»·æ ¼ç›¸å…³æ€§')
    # #
    # #     # æ·»åŠ æ•°å€¼æ ‡ç­¾
    # #     for i in range(len(corr_matrix.columns)):
    # #         for j in range(len(corr_matrix.columns)):
    # #             plt.text(j, i, f"{corr_matrix.iloc[i, j]:.2f}",
    # #                      ha="center", va="center", color="white")
    # #
    # #     # ä¿å­˜å›¾è¡¨
    # #     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # #     chart_path = f"correlation_{timestamp}.png"
    # #     plt.savefig(chart_path)
    # #     plt.close()
    # #
    # #     # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    # #     report = "ğŸ“Š è‚¡ç¥¨ç›¸å…³æ€§åˆ†ææŠ¥å‘Š\n\n"
    # #     report += f"åˆ†æè‚¡ç¥¨: {', '.join(stock_list)}\n"
    # #     report += f"ç›¸å…³æ€§å›¾è¡¨å·²ä¿å­˜è‡³: {chart_path}\n\n"
    # #
    # #     # æ·»åŠ ç›¸å…³æ€§æ‘˜è¦
    # #     report += "ç›¸å…³æ€§æ‘˜è¦:\n"
    # #     for i, stock1 in enumerate(corr_matrix.columns):
    # #         for j, stock2 in enumerate(corr_matrix.columns):
    # #             if i < j:  # é¿å…é‡å¤å’Œè‡ªèº«æ¯”è¾ƒ
    # #                 corr_value = corr_matrix.iloc[i, j]
    # #                 relation = "å¼ºæ­£ç›¸å…³" if corr_value > 0.7 else \
    # #                     "ä¸­åº¦æ­£ç›¸å…³" if corr_value > 0.3 else \
    # #                         "å¼±æ­£ç›¸å…³" if corr_value > 0 else \
    # #                             "å¼±è´Ÿç›¸å…³" if corr_value > -0.3 else \
    # #                                 "ä¸­åº¦è´Ÿç›¸å…³" if corr_value > -0.7 else \
    # #                                     "å¼ºè´Ÿç›¸å…³"
    # #
    # #                 report += f"- {stock1} ä¸ {stock2}: {relation} ({corr_value:.2f})\n"
    # #
    # #     return report + f"\nå›¾è¡¨è·¯å¾„: {chart_path}"
    #
    #
    # # 4. åŸºæœ¬é¢å¯¹æ¯”å·¥å…·
    # # def fundamental_comparison_tool(stock_names: str) -> str:
    # #     """
    # #     å¯¹æ¯”å¤šä¸ªè‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ®
    # #     è¾“å…¥: è‚¡ç¥¨åç§°ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰
    # #     è¾“å‡º: åŸºæœ¬é¢å¯¹æ¯”æŠ¥å‘Š
    # #     """
    # #     stock_list = [name.strip() for name in stock_names.split(",")]
    # #
    # #     if not vector_db:
    # #         return "è‚¡ç¥¨æ•°æ®ç´¢å¼•æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ•°æ®"
    # #
    # #     # æ”¶é›†è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®
    # #     fundamentals = []
    # #     for stock in stock_list:
    # #         if stock not in name_to_code:
    # #             continue
    # #
    # #         # æœç´¢è¯¥è‚¡ç¥¨çš„æœ€æ–°æ–‡æ¡£
    # #         query = f"{stock} {name_to_code[stock]}"
    # #         docs = vector_db.similarity_search(query, k=1)
    # #         if not docs:
    # #             continue
    # #
    # #         doc = docs[0]
    # #         content = doc.page_content
    # #
    # #         # ä»å†…å®¹ä¸­æå–åŸºæœ¬é¢æ•°æ®
    # #         fundamentals.append({
    # #             "name": stock,
    # #             "code": name_to_code[stock],
    # #             "content": content
    # #         })
    # #
    # #     if len(fundamentals) < 2:
    # #         return "éœ€è¦è‡³å°‘ä¸¤æ”¯è‚¡ç¥¨è¿›è¡Œå¯¹æ¯”"
    # #
    # #     # åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š
    # #     report = "ğŸ“‹ è‚¡ç¥¨åŸºæœ¬é¢å¯¹æ¯”æŠ¥å‘Š\n\n"
    # #     report += f"å¯¹æ¯”è‚¡ç¥¨: {', '.join(stock_list)}\n\n"
    # #
    # #     # æ·»åŠ å¯¹æ¯”è¡¨æ ¼
    # #     report += "| é¡¹ç›® | " + " | ".join([f["name"] for f in fundamentals]) + " |\n"
    # #     report += "|-----|" + "|".join(["-----" for _ in fundamentals]) + "|\n"
    # #
    # #     # æå–å¹¶å¯¹æ¯”å…³é”®æŒ‡æ ‡
    # #     indicators = ["ä¸Šå¸‚æ—¥æœŸ", "è‚¡ç¥¨ç±»å‹", "æ¯æ‰‹è‚¡æ•°", "å½“å‰ä»·", "æ¶¨è·Œå¹…", "æˆäº¤é‡"]
    # #
    # #     for indicator in indicators:
    # #         row = f"| {indicator} | "
    # #         for f in fundamentals:
    # #             # åœ¨å†…å®¹ä¸­æœç´¢æŒ‡æ ‡å€¼
    # #             match = re.search(f"{indicator}[ï¼š:]([^ï¼Œã€‚\n]+)", f["content"])
    # #             value = match.group(1).strip() if match else "N/A"
    # #             row += f"{value} | "
    # #         report += row + "\n"
    # #
    # #     report += "\n"
    # #
    # #     # æ·»åŠ ç®€è¦åˆ†æ
    # #     report += "å¯¹æ¯”åˆ†æ:\n"
    # #     # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„åˆ†æé€»è¾‘
    # #     report += "- ä»¥ä¸Šè¡¨æ ¼å±•ç¤ºäº†å„è‚¡ç¥¨çš„å…³é”®åŸºæœ¬é¢æŒ‡æ ‡å¯¹æ¯”\n"
    # #     report += "- è¯·ç»“åˆå…·ä½“æŒ‡æ ‡è¿›è¡Œæ·±å…¥åˆ†æ\n"
    # #
    # #     return report
    #
    #
    # # 5. é£é™©åˆ†æå·¥å…·
    # def risk_analysis_tool(stock_name: str) -> str:
    #     """
    #     åˆ†æå•æ”¯è‚¡ç¥¨çš„é£é™©æŒ‡æ ‡
    #     è¾“å…¥: è‚¡ç¥¨åç§°
    #     è¾“å‡º: é£é™©åˆ†ææŠ¥å‘Š
    #     """
    #     if stock_name not in name_to_code:
    #         return f"æœªè¯†åˆ«çš„è‚¡ç¥¨åç§°: {stock_name}"
    #
    #     if not vector_db:
    #         return "è‚¡ç¥¨æ•°æ®ç´¢å¼•æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ•°æ®"
    #
    #     # æœç´¢è¯¥è‚¡ç¥¨çš„æœ€æ–°æ–‡æ¡£
    #     query = f"{stock_name} {name_to_code[stock_name]}"
    #     docs = vector_db.similarity_search(query, k=5)
    #     if not docs:
    #         return f"æœªæ‰¾åˆ° {stock_name} çš„ç›¸å…³æ•°æ®"
    #
    #     # åˆ†ææ‰€æœ‰ç›¸å…³æ–‡æ¡£
    #     price_changes = []
    #     volumes = []
    #     for doc in docs:
    #         content = doc.page_content
    #
    #         # æå–ä»·æ ¼å˜åŒ–
    #         change_matches = re.findall(r"æ¶¨è·Œå¹…:(-?\d+\.?\d*)%", content)
    #         if change_matches:
    #             price_changes.extend([float(change) for change in change_matches])
    #
    #         # æå–æˆäº¤é‡
    #         volume_matches = re.findall(r"æˆäº¤é‡:([\d,]+)", content)
    #         if volume_matches:
    #             volumes.extend([int(vol.replace(",", "")) for vol in volume_matches])
    #
    #     # è®¡ç®—é£é™©æŒ‡æ ‡
    #     report = f"âš ï¸ {stock_name} é£é™©åˆ†ææŠ¥å‘Š\n\n"
    #
    #     if price_changes:
    #         volatility = np.std(price_changes)
    #         max_drop = min(price_changes)
    #         avg_change = np.mean(price_changes)
    #
    #         report += "ğŸ“ˆ ä»·æ ¼æ³¢åŠ¨åˆ†æ:\n"
    #         report += f"- å¹³å‡æ—¥æ¶¨è·Œå¹…: {avg_change:.2f}%\n"
    #         report += f"- ä»·æ ¼æ³¢åŠ¨ç‡: {volatility:.2f}%\n"
    #         report += f"- æœ€å¤§å•æ—¥è·Œå¹…: {max_drop:.2f}%\n"
    #
    #         if volatility > 5:
    #             report += "  â†’ é«˜é£é™©: ä»·æ ¼æ³¢åŠ¨å‰§çƒˆ\n"
    #         elif volatility > 2:
    #             report += "  â†’ ä¸­é£é™©: ä»·æ ¼æ³¢åŠ¨é€‚ä¸­\n"
    #         else:
    #             report += "  â†’ ä½é£é™©: ä»·æ ¼æ³¢åŠ¨å¹³ç¨³\n"
    #     else:
    #         report += "æœªæ‰¾åˆ°ä»·æ ¼æ³¢åŠ¨æ•°æ®\n"
    #
    #     if volumes:
    #         volume_volatility = np.std(volumes) / np.mean(volumes) * 100
    #         min_volume = min(volumes)
    #         avg_volume = np.mean(volumes)
    #
    #         report += "\nğŸ“Š æˆäº¤é‡åˆ†æ:\n"
    #         report += f"- å¹³å‡æˆäº¤é‡: {avg_volume:,.0f}\n"
    #         report += f"- æˆäº¤é‡æ³¢åŠ¨ç‡: {volume_volatility:.2f}%\n"
    #         report += f"- æœ€ä½æˆäº¤é‡: {min_volume:,.0f}\n"
    #
    #         if volume_volatility > 50:
    #             report += "  â†’ é«˜é£é™©: æˆäº¤é‡æ³¢åŠ¨å‰§çƒˆ\n"
    #         elif volume_volatility > 30:
    #             report += "  â†’ ä¸­é£é™©: æˆäº¤é‡æ³¢åŠ¨æ˜æ˜¾\n"
    #         else:
    #             report += "  â†’ ä½é£é™©: æˆäº¤é‡æ³¢åŠ¨å¹³ç¨³\n"
    #     else:
    #         report += "æœªæ‰¾åˆ°æˆäº¤é‡æ•°æ®\n"
    #
    #     # æ·»åŠ æ€»ä½“é£é™©è¯„ä¼°
    #     report += "\nğŸ” æ€»ä½“é£é™©è¯„ä¼°:\n"
    #
    #     risk_level = "ä½"
    #     if price_changes and volumes:
    #         if volatility > 5 or volume_volatility > 50:
    #             risk_level = "é«˜"
    #         elif volatility > 3 or volume_volatility > 30:
    #             risk_level = "ä¸­"
    #
    #     report += f"- é£é™©ç­‰çº§: {risk_level}\n"
    #     report += "- å»ºè®®: "
    #
    #     if risk_level == "é«˜":
    #         report += "æ­¤è‚¡ç¥¨é£é™©è¾ƒé«˜ï¼ŒæŠ•èµ„éœ€è°¨æ…ï¼Œå»ºè®®å°ä»“ä½æˆ–é¿å…æŠ•èµ„"
    #     elif risk_level == "ä¸­":
    #         report += "æ­¤è‚¡ç¥¨é£é™©é€‚ä¸­ï¼Œå¯è€ƒè™‘é€‚é‡æŠ•èµ„ï¼Œä½†éœ€å¯†åˆ‡å…³æ³¨å¸‚åœºå˜åŒ–"
    #     else:
    #         report += "æ­¤è‚¡ç¥¨é£é™©è¾ƒä½ï¼Œé€‚åˆç¨³å¥å‹æŠ•èµ„è€…"
    #
    #     return report
    #
    #
    #
    #
    # # 6. è¡Œä¸šåˆ†æå·¥å…·
    # # def sector_analysis_tool(sector: str) -> str:
    # #     """
    # #     åˆ†æç‰¹å®šè¡Œä¸šçš„è‚¡ç¥¨è¡¨ç°
    # #     è¾“å…¥: è¡Œä¸šåç§°ï¼ˆå¦‚"ç§‘æŠ€"ã€"é‡‘è"ï¼‰
    # #     è¾“å‡º: è¡Œä¸šåˆ†ææŠ¥å‘Š
    # #     """
    # #     if not vector_db:
    # #         return "è‚¡ç¥¨æ•°æ®ç´¢å¼•æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ•°æ®"
    # #
    # #     # æœç´¢ç›¸å…³è¡Œä¸šè‚¡ç¥¨
    # #     results = vector_db.similarity_search(sector, k=10)
    # #
    # #     if not results:
    # #         return f"æœªæ‰¾åˆ° {sector} è¡Œä¸šçš„ç›¸å…³è‚¡ç¥¨"
    # #
    # #     # æ”¶é›†è¡Œä¸šæ•°æ®
    # #     sector_stocks = []
    # #     for doc in results:
    # #         content = doc.page_content
    # #         stock_name = doc.metadata.get("stock_name", "")
    # #
    # #         # æå–æœ€æ–°ä»·æ ¼å’Œæ¶¨è·Œå¹…
    # #         last_done_match = re.search(r"å½“å‰ä»·:(\d+\.?\d*)", content)
    # #         change_match = re.search(r"æ¶¨è·Œå¹…:(-?\d+\.?\d*)%", content)
    # #
    # #         if last_done_match and change_match:
    # #             sector_stocks.append({
    # #                 "name": stock_name,
    # #                 "code": doc.metadata.get("stock_code", ""),
    # #                 "price": float(last_done_match.group(1)),
    # #                 "change": float(change_match.group(1))
    # #             })
    # #
    # #     if not sector_stocks:
    # #         return f"æœªæ‰¾åˆ° {sector} è¡Œä¸šè‚¡ç¥¨çš„å®Œæ•´æ•°æ®"
    # #
    # #     # åˆ›å»ºè¡Œä¸šåˆ†ææŠ¥å‘Š
    # #     report = f"ğŸ­ {sector} è¡Œä¸šåˆ†ææŠ¥å‘Š\n\n"
    # #     report += f"åŒ…å«è‚¡ç¥¨: {', '.join([s['name'] for s in sector_stocks])}\n\n"
    # #
    # #     # è¡Œä¸šå¹³å‡è¡¨ç°
    # #     avg_change = np.mean([s["change"] for s in sector_stocks])
    # #     best_stock = max(sector_stocks, key=lambda x: x["change"])
    # #     worst_stock = min(sector_stocks, key=lambda x: x["change"])
    # #
    # #     report += "ğŸ“Š è¡Œä¸šæ•´ä½“è¡¨ç°:\n"
    # #     report += f"- å¹³å‡æ¶¨è·Œå¹…: {avg_change:.2f}%\n"
    # #     report += f"- è¡¨ç°æœ€ä½³: {best_stock['name']} ({best_stock['change']:.2f}%)\n"
    # #     report += f"- è¡¨ç°æœ€å·®: {worst_stock['name']} ({worst_stock['change']:.2f}%)\n\n"
    # #
    # #     # åˆ›å»ºè¡¨ç°å›¾è¡¨
    # #     plt.figure(figsize=(10, 6))
    # #     names = [s["name"] for s in sector_stocks]
    # #     changes = [s["change"] for s in sector_stocks]
    # #
    # #     colors = ['green' if c >= 0 else 'red' for c in changes]
    # #     plt.bar(names, changes, color=colors)
    # #
    # #     plt.axhline(y=0, color='gray', linestyle='--')
    # #     plt.title(f"{sector} è¡Œä¸šè‚¡ç¥¨æ¶¨è·Œå¹…")
    # #     plt.ylabel("æ¶¨è·Œå¹… (%)")
    # #     plt.xticks(rotation=45)
    # #     plt.tight_layout()
    # #
    # #     # ä¿å­˜å›¾è¡¨
    # #     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # #     chart_path = f"sector_{sector}_{timestamp}.png"
    # #     plt.savefig(chart_path)
    # #     plt.close()
    # #
    # #     report += f"è¡Œä¸šè¡¨ç°å›¾è¡¨å·²ä¿å­˜è‡³: {chart_path}\n\n"
    # #
    # #     # æ·»åŠ è¡Œä¸šè¶‹åŠ¿åˆ†æ
    # #     report += "ğŸ” è¡Œä¸šè¶‹åŠ¿åˆ†æ:\n"
    # #     if avg_change > 1:
    # #         report += f"- {sector} è¡Œä¸šè¿‘æœŸè¡¨ç°å¼ºåŠ²ï¼Œæ•´ä½“ä¸Šæ¶¨è¶‹åŠ¿æ˜æ˜¾\n"
    # #     elif avg_change < -1:
    # #         report += f"- {sector} è¡Œä¸šè¿‘æœŸè¡¨ç°ç–²è½¯ï¼Œæ•´ä½“ä¸‹è·Œè¶‹åŠ¿æ˜æ˜¾\n"
    # #     else:
    # #         report += f"- {sector} è¡Œä¸šè¿‘æœŸè¡¨ç°å¹³ç¨³ï¼Œæ— æ˜æ˜¾è¶‹åŠ¿\n"
    # #
    # #     report += f"- è¡Œä¸šå†…éƒ¨åˆ†åŒ–: {best_stock['change'] - worst_stock['change']:.2f}ä¸ªç™¾åˆ†ç‚¹\n"
    # #
    # #     return report + f"\nå›¾è¡¨è·¯å¾„: {chart_path}"
    # # print("====", sector_analysis_tool("ç§‘æŠ€"))
    #
    # # 7. æ•°æ®æ›´æ–°å·¥å…·
    # def update_stock_data_tool(stock_names: str) -> str:
    #     """
    #     æ›´æ–°æŒ‡å®šè‚¡ç¥¨çš„æ•°æ®
    #     è¾“å…¥: è‚¡ç¥¨åç§°ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰
    #     è¾“å‡º: æ›´æ–°ç»“æœæŠ¥å‘Š
    #     """
    #     stock_list = [name.strip() for name in stock_names.split(",")]
    #
    #     # è¿™é‡Œéœ€è¦å®ç°å®é™…çš„æ•°æ®æ›´æ–°é€»è¾‘
    #     # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨ä¹‹å‰çš„ add_or_update_stock_data å‡½æ•°
    #
    #     # æ¨¡æ‹Ÿæ›´æ–°è¿‡ç¨‹
    #     results = []
    #     for stock in stock_list:
    #         if stock in name_to_code:
    #             results.append(f"{stock} æ•°æ®å·²æ›´æ–°")
    #         else:
    #             results.append(f"{stock} æœªè¯†åˆ«")
    #
    #     return "æ›´æ–°ç»“æœ:\n- " + "\n- ".join(results)
    #
    # print("====", update_stock_data_tool("è…¾è®¯"))
    #
    #
    # # åˆ›å»ºå·¥å…·åˆ—è¡¨
    # tools = [
    #     Tool(
    #         name="è‚¡ç¥¨æœç´¢",
    #         func=stock_search_tool,
    #         description="æœç´¢è‚¡ç¥¨æ•°æ®ï¼Œè¾“å…¥æŸ¥è¯¢å†…å®¹ï¼Œè¿”å›ç›¸å…³è‚¡ç¥¨ä¿¡æ¯"
    #     ),
    #     Tool(
    #         name="è¶‹åŠ¿åˆ†æ",
    #         func=trend_analysis_tool,
    #         description="åˆ†æè‚¡ç¥¨ä»·æ ¼è¶‹åŠ¿ï¼Œè¾“å…¥è‚¡ç¥¨åç§°ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰å’Œå¤©æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤5å¤©ï¼‰ï¼Œè¿”å›è¶‹åŠ¿æŠ¥å‘Šå’Œå›¾è¡¨"
    #     ),
    #     # Tool(
    #     #     name="ç›¸å…³æ€§åˆ†æ",
    #     #     func=correlation_analysis_tool,
    #     #     description="åˆ†æå¤šä¸ªè‚¡ç¥¨ä¹‹é—´çš„ä»·æ ¼ç›¸å…³æ€§ï¼Œè¾“å…¥è‚¡ç¥¨åç§°ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰ï¼Œè¿”å›ç›¸å…³æ€§æŠ¥å‘Šå’Œå›¾è¡¨"
    #     # ),
    #     # Tool(
    #     #     name="åŸºæœ¬é¢å¯¹æ¯”",
    #     #     func=fundamental_comparison_tool,
    #     #     description="å¯¹æ¯”å¤šä¸ªè‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ®ï¼Œè¾“å…¥è‚¡ç¥¨åç§°ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰ï¼Œè¿”å›å¯¹æ¯”æŠ¥å‘Š"
    #     # ),
    #     Tool(
    #         name="é£é™©åˆ†æ",
    #         func=risk_analysis_tool,
    #         description="åˆ†æå•æ”¯è‚¡ç¥¨çš„é£é™©æŒ‡æ ‡ï¼Œè¾“å…¥è‚¡ç¥¨åç§°ï¼Œè¿”å›é£é™©æŠ¥å‘Š"
    #     ),
    #     # Tool(
    #     #     name="è¡Œä¸šåˆ†æ",
    #     #     func=sector_analysis_tool,
    #     #     description="åˆ†æç‰¹å®šè¡Œä¸šçš„è‚¡ç¥¨è¡¨ç°ï¼Œè¾“å…¥è¡Œä¸šåç§°ï¼ˆå¦‚'ç§‘æŠ€'ã€'é‡‘è'ï¼‰ï¼Œè¿”å›è¡Œä¸šæŠ¥å‘Š"
    #     # ),
    #     Tool(
    #         name="æ•°æ®æ›´æ–°",
    #         func=update_stock_data_tool,
    #         description="æ›´æ–°æŒ‡å®šè‚¡ç¥¨çš„æ•°æ®ï¼Œè¾“å…¥è‚¡ç¥¨åç§°ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰ï¼Œè¿”å›æ›´æ–°ç»“æœ"
    #     )
    # ]
    #
    #
    # # åˆ›å»º Agent
    # def create_agent():
    #     """åˆ›å»ºå¹¶è¿”å›è‚¡ç¥¨åˆ†æ Agent"""
    #     # ç³»ç»Ÿæç¤ºè¯
    #     system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ç¥¨æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿ä½¿ç”¨å„ç§å·¥å…·åˆ†ææ¸¯è‚¡å¸‚åœºæ•°æ®ã€‚
    #     ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥åˆ†æFAISSæ•°æ®åº“ä¸­å­˜å‚¨çš„è‚¡ç¥¨æ•°æ®ï¼š
    #     - è‚¡ç¥¨æœç´¢ï¼šæœç´¢ç›¸å…³è‚¡ç¥¨ä¿¡æ¯
    #     - è¶‹åŠ¿åˆ†æï¼šåˆ†æè‚¡ç¥¨ä»·æ ¼è¶‹åŠ¿
    #     - ç›¸å…³æ€§åˆ†æï¼šåˆ†æå¤šä¸ªè‚¡ç¥¨ä¹‹é—´çš„ä»·æ ¼ç›¸å…³æ€§
    #     - åŸºæœ¬é¢å¯¹æ¯”ï¼šå¯¹æ¯”å¤šä¸ªè‚¡ç¥¨çš„åŸºæœ¬é¢æ•°æ®
    #     - é£é™©åˆ†æï¼šåˆ†æå•æ”¯è‚¡ç¥¨çš„é£é™©æŒ‡æ ‡
    #     - è¡Œä¸šåˆ†æï¼šåˆ†æç‰¹å®šè¡Œä¸šçš„è‚¡ç¥¨è¡¨ç°
    #     - æ•°æ®æ›´æ–°ï¼šæ›´æ–°æŒ‡å®šè‚¡ç¥¨çš„æ•°æ®
    #
    #     è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·è¿›è¡Œåˆ†æã€‚å¦‚æœç”¨æˆ·çš„é—®é¢˜éœ€è¦å¤šä¸ªæ­¥éª¤ï¼Œå¯ä»¥è¿ç»­ä½¿ç”¨å¤šä¸ªå·¥å…·ã€‚
    #
    #     å½“ä½¿ç”¨å›¾è¡¨ç”Ÿæˆå·¥å…·æ—¶ï¼Œè¯·å°†å›¾è¡¨è·¯å¾„åŒ…å«åœ¨å›å¤ä¸­ï¼Œä»¥ä¾¿ç”¨æˆ·æŸ¥çœ‹ã€‚
    #
    #     ä½ çš„åˆ†æåº”è¯¥ä¸“ä¸šã€å…¨é¢ï¼Œå¹¶æä¾›æœ‰ä»·å€¼çš„æŠ•èµ„è§è§£ã€‚
    #     """
    #
    #     # åˆ›å»ºæç¤ºæ¨¡æ¿
    #     prompt = ChatPromptTemplate.from_messages([
    #         ("system", system_prompt),
    #         MessagesPlaceholder("chat_history", optional=True),
    #         ("human", "{input}"),
    #         MessagesPlaceholder("agent_scratchpad")
    #     ])
    #
    #     # é€‰æ‹©LLMæ¨¡å‹
    #     api_key = os.getenv("DEEPSEEK_API_KEY")
    #     # å°è£… OpenAI æˆ–å…¼å®¹ API æˆä¸ºä¸€ä¸ª LangChain å¯ç”¨çš„è¯­è¨€æ¨¡å‹æ¥å£ï¼ˆLLMï¼‰
    #     llm = ChatOpenAI(
    #         api_key=api_key,
    #         base_url="https://api.deepseek.com",
    #         model="deepseek-chat",
    #         temperature=0.7  # æ§åˆ¶è¯­è¨€æ¨¡å‹è¾“å‡ºå†…å®¹ éšæœºæ€§ çš„å‚æ•°ï¼Œå®˜æ–¹æ¨è0.7
    #     )
    #
    #     # åˆ›å»ºAgent
    #     agent = create_openai_functions_agent(llm, tools, prompt)
    #
    #     # åˆ›å»ºæ‰§è¡Œå™¨
    #     agent_executor = AgentExecutor(
    #         agent=agent,
    #         tools=tools,
    #         verbose=True,
    #         return_intermediate_steps=True
    #     )
    #
    #     return agent_executor
    #
    #
    # # åˆ›å»ºAgentå®ä¾‹
    # stock_analyst_agent = create_agent()
    #
    #
    # # ä¸»å‡½æ•° - ç”¨æˆ·äº¤äº’
    # def main():
    #     print("æ¬¢è¿ä½¿ç”¨æ¸¯è‚¡æ•°æ®åˆ†æåŠ©æ‰‹ï¼è¾“å…¥'é€€å‡º'ç»“æŸå¯¹è¯ã€‚")
    #     print("æ”¯æŒçš„åˆ†æç±»å‹ï¼šè¶‹åŠ¿åˆ†æã€ç›¸å…³æ€§åˆ†æã€åŸºæœ¬é¢å¯¹æ¯”ã€é£é™©è¯„ä¼°ã€è¡Œä¸šåˆ†æç­‰\n")
    #
    #     while True:
    #         user_input = input("\næ‚¨æƒ³åˆ†æä»€ä¹ˆï¼Ÿ\n> ")
    #
    #         if user_input.lower() in ["é€€å‡º", "exit", "quit"]:
    #             print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
    #             break
    #
    #         try:
    #             # æ‰§è¡ŒAgent
    #             result = stock_analyst_agent.invoke({"input": user_input})
    #
    #             # æ˜¾ç¤ºç»“æœ
    #             print("\n" + "=" * 60)
    #             print("ğŸ“Š åˆ†æç»“æœ:")
    #             print(result["output"])
    #             print("=" * 60)
    #
    #             # æ˜¾ç¤ºä¸­é—´æ­¥éª¤ï¼ˆå¯é€‰ï¼‰
    #             if result.get("intermediate_steps"):
    #                 print("\nğŸ”§ åˆ†ææ­¥éª¤:")
    #                 for step in result["intermediate_steps"]:
    #                     action = step[0]
    #                     observation = step[1]
    #                     print(f"- æ“ä½œ: {action.tool}ï¼ˆè¾“å…¥: {action.tool_input}ï¼‰")
    #                     print(f"  ç»“æœ: {observation[:200]}...")
    #
    #         except Exception as e:
    #             print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
    #
    #
    # if __name__ == "__main__":
    #     main()
    embedder = pipeline(
        "feature-extraction",
        model=MODEL_PATH,
        device=0 if torch.cuda.is_available() else -1,  # 0 è¡¨ç¤ºç¬¬ä¸€ä¸ª GPU
        local_files_only=True
    )

    # è·å–åµŒå…¥
    result = embedder("æµ‹è¯•æ–‡æœ¬")
    embeddings = result[0][0]  # è·å–ç¬¬ä¸€ä¸ª token çš„åµŒå…¥ï¼ˆé€šå¸¸æ˜¯ [CLS] tokenï¼‰

    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼åµŒå…¥ç»´åº¦: {len(embeddings)}")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")